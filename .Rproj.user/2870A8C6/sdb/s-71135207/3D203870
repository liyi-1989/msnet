{
    "collab_server" : "",
    "contents" : "##########################################################################\n#  R implementation of the structured elastic net\n#  'Feature Selection Guided by Structural Information'\n#\n#  Slawski, M., zu Castell, W., Tutz, G.\n#  The Annals of Applied Statistics, 4(2), 1056-1080 \n#  \n#  Optimization is based on FISTA:\n#  A. Beck and M. Teboulle \n#   'A Fast Iterative Shrinkage-Thresholding Algorithm for\n#    Linear Inverse Problems' \n#  SIAM J. on Imaging Sciences, 2(1), 2009\n#  \n#  Any loss functions which is continuously\n#  differentiable can be run with the code template provided\n#  here. Squared loss and logistic loss are already implemented.\n#\n#  Arguments: \n#  X --- design matrix, including penalized and unpenalized (such\n#        as the constant term) features\n#  y --- response\n#  Lambda ---  structure matrix for the quadratic regularizer\n#  lossfun --- a continuously differentiable loss function\n#              s. the examples below of how to implement\n#              them\n#  gradfun --- a function that evaluates the gradient of the\n#              loss function\n#  lambda1 --- regularization parameter for the ell_1-term\n#  lambda2 --- regularization parameter for the quadratic term\n#  stepsize --- either 'fixed' or 'backtracking'.\n#               If 'stepsize == fixed' an upper bound on the\n#               Lipschitz constant of the gradient has to\n#               be provided. (s. 'control' below)\n#  control: a list of OPTIONAL additional arguments\n#  weights: feature-wise weights for the ell_1-term.\n#           Weights equal to zero lead to removal of the\n#           ell_1-penalty for the respective feature.\n#           Can also be used for an adaptive version\n#           of the structured elastic net.\n#  L: Lipschitz constant of the gradient.\n#  use.gram: whether the Gram matrix t(X) %*% X\n#            should be computed stored.\n#  maxiter: an upper bound on the number of iterations.  \n#  tol: If successive iterates differ less than tol\n#       (w.r.t. ell_2-norm), the algorithm stops.\n#  init: starting value. Default is the zero vector.\n#  sigma: parameter for back-tracking.\n#\n#   Value:\n#\n#   A list containing the following elements. \n#   beta: The estimate. \n#   iter: number of iterations run\n#   obj: objective values of the iterates\n#   objbetahat: the objective value of the estimate betahat.\n#   kkt: KKT optimality\n#   tol: difference of the last two successive iterates\n#        (w.r.t. ell_2-norm)\n#    L: Lipschitz constant of the gradient or an estimate thereof\n#       (if stepsize was set to 'backtracking'). \n#  (C) Martin Slawski, Feb. 2012\n#  Please report bugs to martin.dot.slawski (at) googlemail.com\n##########################################################################\nsenet_fista <- function(X, y, Lambda, lossfun, gradfun, lambda1, lambda2,\n                        stepsize = c(\"fixed\", \"backtracking\"),\n                        control = list(weights = 1,\n                                       L = NULL,\n                                       use.gram = TRUE,\n                                       maxiter = NULL, \n                                       tol = 1e-8,\n                                       init = NULL,\n                                       sigma = 0.9)){\n#### input checking\n  p <- ncol(X)\n  n <- nrow(X)\n  if(length(y) != nrow(X))\n    stop(\"Dimensions of 'X' and 'y' do not match \\n\")\n  if(!all(dim(Lambda) == p))\n    stop(\"Dimensions of X and Lambda do not match \\n\")\n  \n  if(!all(c(lambda1, lambda2) >= 0))\n    stop(\"Regularization parameters lambda1, lambda2 have\n          to be non-negative \\n\")\n  \n  #### un-pack\n  stepsize <- match.arg(stepsize)\n  \n  weights <- control$weights\n  \n  ###\n  \n  if(is.null(weights)){ \n  \n    weights <- rep(1, p) \n    \n  }\n  \n  else{\n  \n    if(length(weights) == 1)\n       weights <- rep(weights, p)\n    else\n       if(!(length(weights) == p))\n          stop(\"The length of 'weights' has to be equal to the number of columns of X \\n\")\n          \n     if(any(weights < 0))\n      stop(\"The entries of weights have to be non-negative \\n\")     \n  }\n  \n  ###\n  L <- control$L\n  if(is.null(L)){\n    if(stepsize == \"fixed\"){ \n      cat(\"Lipschitz constant L unspecified; 'stepsize' set to 'backtracking' \\n\")\n      stepsize <- \"backtracking\"\n    }\n  }\n  use.gram <- control$use.gram\n  if(is.null(use.gram)){\n    use.gram <- FALSE\n  }\n  else\n    XtX <- crossprod(X)\n  maxiter <- control$maxiter\n  if(is.null(maxiter))\n    maxiter <- Inf\n    \n  tol <- control$tol\n  if(is.null(tol))\n    tol <- 1e-8\n  init <- control$init\n  if(!is.null(init)){\n    if(!length(init) == p){\n      warning('Dimension of initial solution does not match \\n')\n      \n    }\n    else\n      beta <- init\n  }\n  else\n    beta <- rep(0, p) ### initalize as 0-vector. \n    \n  sigma <- control$sigma\n  if(is.null(sigma)){\n    sigma <- 0.9\n    \n  }  \n  gamma <- beta  ### for the extrapolation steps.\n  eta <- X %*% gamma\n  Lambdatimesgamma  <- lambda2 * (Lambda %*% gamma)\n  objs <- lossfun() +  (t(gamma) %*% Lambdatimesgamma) + lambda1 * sum(abs(gamma) * weights) ### objective\n  eps <- .Machine$double.eps ### machine precision\n  ###\n  Xty <- t(X) %*% y\n  \n  ### KKT optimality\n  kktvec <- gradfun() +  2 * Lambdatimesgamma\n  A <- abs(gamma) > eps\n  if(sum(A) > 0)\n    kktA <- max(abs(kktvec[A] - lambda1 * weights[A]))\n  else\n    kktA <- 0\n  if(sum(!A) > 0)\n    kktAc <- max(pmax(abs(kktvec[!A]) - lambda1 * weights[!A], 0))\n  else\n    kktAc <- 0\n  kktopt <- max(kktA, kktAc)\n  ###\n  betahat <- gamma ### the iterate to be returned.\n  ###\n  k <- 0 ### iteration counter\n  stopcond <- FALSE\n  t <- 1 ### initialization of stepsize (if back-tracking is used)\n  \n  ### helper function: soft thresholding operator\n  soft <- function(x, alpha) pmax(abs(x) - alpha, 0) * sign(x)\n  \n  ### ** End of initialization steps ** ###\n  ### Main FISTA algorithm\n  while(!stopcond){\n    if(k>10){\n      if( (k%%floor(maxiter/10)==0)){\n        cat(\"Iteration \",k,\"...\\n\")\n      }\n    }\n\n    ### evaluate gradient\n    grad <- gradfun() + 2 * lambda2 * Lambda %*% gamma\n    ###\n     \n    \n    if(stepsize == \"fixed\"){\n      betanew <- soft(gamma - grad/L, lambda1 * weights/ L)\n      eta <- X %*% betanew\n      lossbetanew <- lossfun()\n      ####\n      Lambdatimesbetanew <- lambda2 * (Lambda %*% betanew)\n      kktvec <- gradfun() +  2 * Lambdatimesbetanew\n      \n      A <- abs(betanew) > eps\n       if(sum(A) > 0)\n         kktA <- max(abs(kktvec[A] - lambda1 * weights[A]))\n       else\n         kktA <- 0\n      if(sum(!A) > 0)\n        kktAc <- max(pmax(abs(kktvec[!A]) - lambda1 * weights[!A], 0))\n      else\n        kktAc <- 0\n      kktopt <- c(kktopt, max(kktA, kktAc))\n      \n      ###\n      objnew <- lossbetanew + t(betanew) %*% Lambdatimesbetanew + lambda1 * sum(abs(betanew) * weights)\n      if(objnew < min(objs))\n        betahat <- betanew\n      objs <- c(objs, objnew)\n      \n    }\n    else{ ### back-tracking\n      lossgamma  <-  lossfun()\n      flag <- 1\n      \n        m <- 0\n        while(flag  && t > eps){\n            t <- t * sigma^m\n            betanew <- soft(gamma - t * grad, lambda1 * weights * t)\n            eta <- X %*% betanew\n            lossbetanew  <- lossfun() \n            if(lossbetanew < lossgamma + sum(grad * (betanew - gamma)) + 1/(2* t) * sum((betanew - gamma)^2))\n                flag <- 0\n            else\n                m <- m + 1\n            \n        }\n        ####\n        Lambdatimesbetanew <- lambda2 * (Lambda %*% betanew)\n        kktvec <- gradfun() +  2 * Lambdatimesbetanew\n      \n        A <- abs(betanew) > eps\n      if(sum(A) > 0)\n         kktA <- max(abs(kktvec[A] - lambda1 * weights[A]))\n       else\n         kktA <- 0\n      if(sum(!A) > 0)\n        kktAc <- max(pmax(abs(kktvec[!A]) - lambda1 * weights[!A], 0))\n      else\n        kktAc <- 0\n      kktopt <- c(kktopt, max(kktA, kktAc))\n        \n        ####\n        objnew <- lossbetanew + t(betanew) %*% Lambdatimesbetanew + lambda1 * sum(abs(betanew) * weights)\n         if(objnew < min(objs))\n           betahat <- betanew\n      objs <- c(objs, objnew)\n           \n    }\n    gamma <- betanew + k/(k + 3) * (betanew - beta)\n    delta <- sqrt( sum((beta - betanew)^2))\n    if( delta < tol){\n      beta <- betanew\n      stopcond <- TRUE \n    }\n    beta <- betanew\n    k <- k + 1\n    if(k > maxiter)\n      stopcond <- TRUE\n   eta <- X %*% gamma\n      \n  }  \n    \n  ### Return output\n  out <- list()\n  out$beta <- betahat\n  out$iter <- k\n  out$obj <- objs\n  out$objbetahat <- min(objs)\n  out$kkt <- kktopt\n  out$tol  <- delta\n  if(stepsize == \"fixed\")\n    out$L <- L\n  else\n   out$L <- 1/t\n  return(out)\n  ###\n} \n###################################################################  \n### Example 1: squared loss\n  \nl2loss <- function() sum((get(\"y\", parent.frame(1)) - get(\"eta\", parent.frame(1)))^2)\nl2grad <- function(){\n  if(get(\"use.gram\", parent.frame(1))){\n    2 * (get(\"XtX\", parent.frame(1)) %*% get(\"beta\", parent.frame(1)) - get(\"Xty\", parent.frame(1)) )\n  }   \n  else{\n    2 * (t(get(\"X\", parent.frame(1))) %*% get(\"eta\", parent.frame(1)) - get(\"Xty\", parent.frame(1)))\n }   \n}\n### Example 2: logistic loss\nlogisticloss <- function(){\n  - sum(get(\"y\", parent.frame(1)) *  get(\"eta\", parent.frame(1)) - log(1 + exp(get(\"eta\", parent.frame(1)))))\n}\n  \nlogisticgrad <- function(beta, use.gram){\n  mu <- plogis( get(\"eta\", parent.frame(1)) )\n  t(get(\"X\", parent.frame(1))) %*% mu - get(\"Xty\", parent.frame(1))\n}\n### ... add a continuously differentiable loss function\n##      with Lipschitz-continuous gradient of your\n###     choice here ...\n###################################################################\n### helper functions\n### computation of Lipschitz constant for squared loss\nLl2 <- function(X, Lambda, lambda2){\n  #2 * (max( svd(crossprod(X) + lambda2 * Lambda)$d )^2) # this svd method seems slow\n  2*base::norm(crossprod(X)+ lambda2 * Lambda,\"2\")^2\n  \n}\n### computation of Lipschitz function for logistic loss \nLlogistic <- function(X, Lambda, lambda2){\n    ( 0.25 * max( svd(crossprod(X) + lambda2 * Lambda)$d )^2)\n  \n  \n}\n###################################################################\n###################################################################\n  \n### 1st example: two bumps\n\n### \nsimblock1d <- function(n, p=100, noise.signal = 0.25, noise.response = 30, beta,...){\n  Xt <- matrix(ncol = p, nrow = n)\n  for(i in 1:n){\n    bi <- runif(5, 0, 5)\n    mi <- runif(5, 0, 2*pi)\n    Xt[i,]  <- sapply(1:p, function(z) sum(bi * sin(z*pi*(5-bi)/50-mi))) + rnorm(p, sd=noise.signal)\n  }\n  y <- Xt %*% beta + rnorm(n, sd=noise.response)\n  return(list(Xt = Xt, y=y, beta = beta))\n}\nfd <- function(p){\n  D <- matrix(nrow = p-1, ncol = p, data = 0)\n  for(i in 1:(p-1)){\n    D[i,i] <- 1\n    D[i,i+1] <- -1\n  }\n  D\n} \n\ntwobump=function(t,px){\n  (floor(0.2*px) < t && t < floor(0.4*px))*((-1)*(floor(0.3*px) - t)^2+px)/(2*px) +\n  (floor(0.6*px) < t && t < floor(0.8*px))*((floor(0.7*px) - t)^2-px)/(2*px)\n}\n\n# a=sapply(1:px, function(t) (20 < t && t < 40)*((-1)*(30 - t)^2+100)/200 +\n#          (60 < t && t < 80)*((70 - t)^2-100)/200)\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1482848942077.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1559027233",
    "id" : "3D203870",
    "lastKnownWriteTime" : 1483021493,
    "last_content_update" : 1483021493049,
    "path" : "D:/works/msenet/senet.R",
    "project_path" : "senet.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}